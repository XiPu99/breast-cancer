---
title: "STA360 Final Project Report"
author: "Tong Wu, Xi Pu"
date: "4/15/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
```

```{r packages}
library(corrplot)
library(rstanarm)
library(patchwork)
library(bayesplot)
library(caret)
library(loo)
library(tidyverse)
library(e1071)
library(pROC)
```

## Introduction

Breast cancer is one of the most prominent forms of cancer in the US and one of the deadliest besides lung cancer. In fact, about 1 in 8 US women will develop breast cancer in the course of their lifetime and the around 40,000 women are expected to die from breast cancer in 2020. Diagnosis of breast cancer involves an initial screening with mammogram for a tumor, followed by a biopsy of the cancerous tissue. Pathologists typically analyze the tissue, noting its size, shape, texture, and color, before determining whether the tumor is benign (harmless), or malignant (has metastasized). However, demands for automating this classification for basic tumors is rising as physicians are inundated with tests after tests in an aging US population. To improve efficiency of breast cancer diagnosis, it is good to develop a model that could analyze measurements regarding the tumor taken from images of these slides and accurately predict whether the tumor is benign or malignant. 

To address this need, we will use the data from the Breast Cancer Wisconsin (Diagnostic) Data Set to develop a Bayesian logistic regression model that can accurately predict whether a tumor is benign or malignant, given the characteristic outputs provided by the image measurements. We will first generate a posterior logistic regression model including all 31 covariates present in the dataset, setting our prior initial estimates for the coefficients as the sample mean for each covariate. For the model generation, we plan on using MCMC sampling to determine the posterior coefficients. Since the data contains many highly correlated covariates, we will calculate the VIF for all the predictors, determine if there is a significant difference in M/B diagnostics for each predictor based on boxplots, and use both information to choose the most suitable predictors and remove the high VIF, non-significant predictors. Afterwards, we will update our model with only the significant predictors and perform the Bayesian analysis again to obtain the new posterior coefficients with confidence intervals. Next, we will evaluate our MCMC model to determine if there is good convergence and mixing. Finally, we will test this model's performance on our training data and through leave one out cross-validation test. We will determine our accuracy by generating confusion matrices for the training dataset and cross-validatino test. Our goal is to develop an accurate logistic regression model that can accurately predict whether a tumor is malignant or benign given measurements in the Wisconsin dataset. This would help improve the efficiency of basic M/B diagnosis for breast cancer by automating the classification process. 

## Data

#### Data source

The dataset used for this project is the Wisconsin Diagnostic Breast Cancer Dataset. It can be accessed from this link https://www.kaggle.com/uciml/breast-cancer-wisconsin-data and can also be found on [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29). All the data in the dataset is already in clean and workable format and there are no missing values present in the dataset.

This dataset has 569 observations and 32 variables. Each observation contains information about characteristics of the cell nuclei present in a digitized image of a fine needle aspirate (FNA) of a breast mass.
 
#### Data description

Variable Information:
1. ID number
2. Diagnosis: a binary response variable that either takes value M or B (M = malignant, B = benign)

__3-32)__

Ten numeric values are computed for each cell nucleus:
  1. radius (mean of distances from center to points on the perimeter)
  2. texture (standard deviation of gray-scale values)
  3. perimeter
  4. area
  5. smoothness (local variation in radius lengths)
  6. compactness (perimeter^2 / area - 1.0)
  7. concavity (severity of concave portions of the contour)
  8. concave points (number of concave portions of the contour)
  9. symmetry
  10. fractal dimension ("coastline approximation" - 1)
The mean, standard error and "worst" or largest (mean of the three
largest values) of these features were computed for each image,
resulting in 30 variables. For instance, field 3 is Mean Radius, field
13 is Radius SE, field 23 is Worst Radius.

#### Exploratory data analysis

For exploratory data analysis, we created a side-by-side boxplot for each covariate to see if there is any significant difference in the distribution of the covariate between malignant and benigh tumors. This helps us identify potentially significant predictors.

Since there are lots of covariates to plot, we chose to display a few ones that show significant difference: 
```{r eda}
data <- read.csv("data.csv")
area_plot <- data %>% ggplot(mapping = aes(y = area_mean, x = diagnosis)) + 
  geom_boxplot(alpha = 0.5) +
  theme_bw()

sym_plot <- data %>% ggplot(mapping = aes(y = symmetry_mean, x = diagnosis)) + 
  geom_boxplot(alpha = 0.5) +
  theme_bw()

txt_plot <- data %>% ggplot(mapping = aes(y = texture_mean, x = diagnosis)) + 
  geom_boxplot(alpha = 0.5) +
  theme_bw()

con_plot <- data %>% ggplot(mapping = aes(y = concavity_mean, x = diagnosis)) + 
  geom_boxplot(alpha = 0.5) +
  theme_bw()

(area_plot + sym_plot) / (txt_plot + con_plot)
```


For other covariates, the boxplots do not show significant difference between malignant and benigh tumors. Some examples are shown below:

```{r}
data %>% ggplot(mapping = aes(y = texture_se, x = diagnosis)) + 
  geom_boxplot(alpha = 0.5) +
  theme_bw() + data %>% ggplot(mapping = aes(y = symmetry_se, x = diagnosis)) + 
  geom_boxplot(alpha = 0.5) +
  theme_bw() + data %>% ggplot(mapping = aes(y = smoothness_se, x = diagnosis)) + 
  geom_boxplot(alpha = 0.5) +
  theme_bw() + data %>% ggplot(mapping = aes(y = fractal_dimension_se, x = diagnosis)) + 
  geom_boxplot(alpha = 0.5) +
  theme_bw()
```

Another interesting pattern we observed is that boxplots for variables with the "se" suffix do not show as much difference between malignant and benigh tumors compared to the corresponding variabless with the "mean" suffix. This implies that the standard error data is less informative than the mean data in terms of distinguish between a malignant and benigh tumor.

Itâ€™s also important to point out that since 20 of the 30 predictors were computed from data, high multicollinearity exists in this dataset. In addition, some variables are directly computed from other variables. For example, both `area_mean` and `perimeter_mean` are calculated by using `radius_mean`. To examine this issue further, we created a correlation plot as shown below.

```{r fig.height=7}
corrplot(cor(data[,-1:-2]), method = "color", tl.col="black", tl.srt=45)
```

As we can see from the plot above, some predictors are highly correlated as expected. So we should not include all the predictors in our model due to multicollinearity. One way we can deal with this is transforming the predictors using principal component analysis(PCA). We can then perform logistic regression on PCA transformed variables. However, the problem is that the model is not interpretable if we use PCA transformed variables. One of the biggest advantage of logistic regression is interpretability. By using PCA variables, we lose that advantage. So, instead, we use the variance inflation factor (VIF) as the criteria to remove variables that cause multicollinearity.

## Model

#### Model specification

Since the variable we are interested in predicting in this problem is a binary variable, we chose to perform a Bayesian analysis of the coefficients for the covariates in a logistic regression model that can predict whether a tumor is malignant or benigh. Our sampling probability model will be IID Bernouli distributions because the response variable (diagnosis) is binary: we assume that the binary response variable $Y_i$ follows a Bernoulli distribution with probability of success $p_i$ (success here means that the breast cancer case is malignant in the context of the problem). 

We use the logit function as our link function to connect our model parameter $p_i$ with the systematic component $\eta$:

$$
\text{logit}(p_i) = \log(\frac{p_i}{1-p_i})
$$

With this link function, we can then write our systematic component/regression model as following:

$$
\eta = \text{logit}(p_i) = \boldsymbol{\beta}^T \boldsymbol{X} = \beta_1X_1 + \dots + \beta_nX_n \\
\text{where }\boldsymbol{\beta} \text{ are the regression coefficients and }\boldsymbol{X} \text{ are the covariates}
$$

With all three ingredients (probability model, link function, and systematic component) ready, we can proceed to prior specification and MCMC posterior inference of this logistic regression model.

#### Prior specification

The first step is choosing a prior distribution for the set of regression coefficient parameters $\boldsymbol{\beta}$. We set our prior estimate for the coefficients as a vector of t-distribution's centered around 0 with 1 degree of freedom. The prior estimates are centered around zero because we do not have enough information to knnow whether each covariate is a significant predictor for the diagnosis. We chose a t-distribution over a normal distribution because t-distribution is more spread out, which reflects the fact that we don't have enough prior information to say that the coefficient is close to 0. With the same reason, we set the degree of freedom to be 1, a small number, to represent that we have a weak prior. Once we've set the prior distribution, we obtain the posterior distribution for the coefficients via MCMC sampling. 
Below is the posterior distribution that `stan_glm` will draw from when using MCMC.

```{r cache=TRUE}
SEED=20200425
t_prior <- student_t(df = 1, location = 0, scale = 5)
post1 <- stan_glm(diagnosis ~ ., data = data[,-1],
                 family = binomial(link = "logit"), 
                 prior = t_prior, prior_intercept = t_prior, QR=TRUE,
                 seed = SEED, iter = 2000, chain = 1, refresh = 0)
```

Since the data has many highly correlated predictors, we sought to deal with issues of multicolinearity by removing predictors based off high VIF values. Below is VIF values for each covariate. 

```{r}
car::vif(post1)
```


```{r cache=TRUE}
#post2 <- stan_glm(diagnosis ~ symmetry_mean + texture_se +
                    #smoothness_se + symmetry_se + smoothness_mean, data = data[,-1],
                 #family = binomial(link = "logit"), 
                 #prior = t_prior, prior_intercept = t_prior, QR=TRUE,
                 #seed = SEED, iter = 2000, chain = 1, refresh = 0)
```

Taking into account the significant differences in the boxplots for each covariate, as well as their VIF's, we chose 4 predictors that had significant difference between malignant vs benigh and had low vif values: symmetry_mean, texture_mean, area_mean, concavity_mean. Below is the posterior distribution for those four predictors generated from MCMC sampling.


```{r}
post3 <- stan_glm(diagnosis ~ symmetry_mean + texture_mean +
                    area_mean + concavity_mean, data = data[,-1],
                 family = binomial(link = "logit"), 
                 prior = t_prior, prior_intercept = t_prior, QR=TRUE,
                 seed = SEED, iter = 2000, chain = 1, refresh = 0)
```

Below is the model coefficients and CI:
```{r}
round(coef(post3), 2)
round(posterior_interval(post3, prob = 0.9), 2)
car::vif(post3)
```

This is the posterior coefficients for the final model logistic regression model generated from our Bayesian analysis. Now we will look at model diagnostics.

#### Model diagnostics

We wanted to evaluate the performance of our MCMC model. Thus, we check for mixing and convergence. 

```{r}
#color_scheme_set("red")
#mcmc_trace(post2, pars = c("texture_se", "symmetry_mean", "smoothness_se", "smoothness_mean"))
```

```{r}
#mcmc_acf(post2, pars = c("texture_se", "symmetry_mean", "smoothness_se", "smoothness_mean"), lags = 10)
```

```{r}
color_scheme_set("red")
mcmc_trace(post3, pars = c("texture_mean", "symmetry_mean", "area_mean", "concavity_mean"))
```

```{r}
mcmc_acf(post3, pars = c("texture_mean", "symmetry_mean", "area_mean", "concavity_mean"), lags = 10)
```

The traceplots for the coefficient of the four covariates indicates that there is immediate convergence and good mixing. This is backed up by the autocorrelation plots which show the autocorrelations greatly reduce between consecutive samples after only a few iterations. Thus, our model appears to have good performance with little to no burn-in before convergence as well as good mixing. 

## Results

#### Posterior predictive diagnostics

For posterior predictive checks, we used the 1000 posterior samples generated during the MCMC sampling to get our final prediction. Since each posterior sample contains predictions for all 569 observations, for each observation, we calculated the proportion of samples that are predicted to be malignant. If the proportion is above 0.5, then we conclude that this observation is a malignant case and benign otherwise. We can then generate a confusion matrix based on the final prediction to evaluate the performance of our model. Furthermore, we generated an ROC curve based on the training data. 

```{r}
pred <- posterior_predict(post3)
pr <- factor(as.integer(colMeans(pred) >= 0.5))
p <- colMeans(pred)
levels(pr) <- c("B", "M")
caret::confusionMatrix(pr, data$diagnosis)[2]
caret::confusionMatrix(pr, data$diagnosis)$overall["Accuracy"]
t <- as.integer(data$diagnosis)-1
par(pty="s")
roc(t, p, plot=TRUE, legacy.axes=TRUE)
```

The accuracy of our predictions is about 92.8%, and the area under the curve was .9814. Even though our posterior predictive analysis shows that the model gives us very accurate predictions, there might be overfitting issues since we are testing on the training set, which is the dataset we used to fit our model earlier. So, to further test our posterior predictions and see if overfitting exists, we decided to use the `loo` package to perform a leave-one-out cross validation test.

```{r message=FALSE, warning=FALSE}
loo1 <- loo(post3, save_psis = TRUE)
ploo <- E_loo(pred, loo1$psis_object, type="mean", log_ratios = -log_lik(post3))$value
#round(mean(xor(ploo>0.5,as.integer(y==0))),2)
mean(xor(ploo>0.5,as.integer(data$diagnosis=="B")))
#mean( (ploo>0.5 & as.integer(data$diagnosis=="M")) | (ploo<0.5 & as.integer(data$diagnosis=="B")) )
```

The result shows an accuracy of 92.6%, which is very close to the accuracy we got earlier when we tested the predictions using the training set. This indicates that our model indeed is a good fit for accurately predicting malignant vs benigh tumors. 

#### Model interpretation


```{r}
round(coef(post3), 2)
round(posterior_interval(post3, prob = 0.9), 2)
car::vif(post3)
```


From the model coefficient and CI outputs, we see that:
Intercept: At baseline when all other covariates are set to zero, the log odds of malignant over benigh tumor is -21.66 with 90% CI (-26.27, -17.78). This makes sense because we expect that a tumor with no area, texture, symmetry, and concavity would not appear to be a tumor at all and thus would be classified as benigh. 
Symmetry: Holding all else constant, for every unit increase in symmetry_mean, the log odds that the tumor is malignant over benigh is expected to increase by 31.52, which 90% CI (16.94, 47.23). This suggests that the symmetry of the tumor is associated with a big role in diagnosing malignant tumors or benigh tumors. 
Texture: Holding all else constant, for every unit increase in the mean texture (deviation from the gray scale), the log odds that the tumor is malignant over benigh is expected to increase by .28 with 90% CI (.21, .37). The low magnitude of the slope suggests that texture does not play as big of a role in favoring the odds of malignant over benigh diagnosis.
Area: Holding all else constant, for every unit increase in the area of the tumor, the log odds that the tumor is malignant over benigh is expected to increase by 0.01 with a 90% CI (.01, .02). The magnitude of this slope is significant but very close to 0, indicating that area of the tumor does not play a big role in favoring the odds of malignant over benigh diagnosis. 
Concavity: Holding all else constant, for every unit increase in the tumor's mean of severity of concave portions of the contour, the log odds that the tumor is malignant over benigh is expected to increase by 21.44, with 90% CI (15.33, 27.85). The large magnitude of this slope suggests that the severity of concave portions of the tumor plays a big role in favoring the odds of diagnosing malignant over benigh for a tumor. 

## Conclusion

We generated a logsitic regression model using Bayesian analysis and were able predict the diagnosis of the breast tumors with great accuracy. Our model was able to predict our training data with 92.8% accuracy according to the confusion matrix. When leave one out cross validation test was performed, our model still performed accurately, with 92.6% accuracy based on the confusion matrix. 
Based on the posterior coefficients for the predictors generated, we can see that the more symmetric and more concavity a breast tumor is, the more likely it is expected to be a malignant diagnosis. These two characteristics had strong influence on whether the tumor is malignant or benigh. Increases in area of the tumor and deviation from the gray scale indicating greater texture also play a smaller, but still significant role in tipping the odds towards malignant diagnosis over benigh. 
Overall, these findiings demonstrate that we have succeeded in achieving our goal of developing a logisitc regression model that can accurately predict the diagnosis of breast cancer tumor masses in the Wisconsin Dataset. We hope this model has the potential to improve the efficiency of basic breast cancer diagnosis.






