---
title: "Final project report"
author: "Tong Wu, Xi Pu"
date: "4/15/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
```

```{r packages}
library(corrplot)
library(rstanarm)
library(patchwork)
library(bayesplot)
library(caret)
library(loo)
library(tidyverse)
```

## Introduction


## Data

#### Data source

The dataset used for this project is the Wisconsin Diagnostic Breast Cancer Dataset. It can be accessed from this link https://www.kaggle.com/uciml/breast-cancer-wisconsin-data and can also be found on [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29). All the data in the dataset is already in clean and workable format and there are no missing values present in the dataset.

This dataset has 569 observations and 32 variables. Each observation contains information about characteristics of the cell nuclei present in a digitized image of a fine needle aspirate (FNA) of a breast mass.
 
#### Data description

Variable Information:
1. ID number
2. Diagnosis: a binary response variable that either takes value M or B (M = malignant, B = benign)

__3-32)__

Ten numeric values are computed for each cell nucleus:
  1. radius (mean of distances from center to points on the perimeter)
  2. texture (standard deviation of gray-scale values)
  3. perimeter
  4. area
  5. smoothness (local variation in radius lengths)
  6. compactness (perimeter^2 / area - 1.0)
  7. concavity (severity of concave portions of the contour)
  8. concave points (number of concave portions of the contour)
  9. symmetry
  10. fractal dimension ("coastline approximation" - 1)
The mean, standard error and "worst" or largest (mean of the three
largest values) of these features were computed for each image,
resulting in 30 variables. For instance, field 3 is Mean Radius, field
13 is Radius SE, field 23 is Worst Radius.

#### Exploratory data analysis

```{r eda}
data <- read.csv("data.csv")
area_plot <- data %>% ggplot(mapping = aes(y = area_mean, x = diagnosis)) + 
  geom_boxplot(alpha = 0.5) +
  theme_bw()

sym_plot <- data %>% ggplot(mapping = aes(y = symmetry_mean, x = diagnosis)) + 
  geom_boxplot(alpha = 0.5) +
  theme_bw()

txt_plot <- data %>% ggplot(mapping = aes(y = texture_mean, x = diagnosis)) + 
  geom_boxplot(alpha = 0.5) +
  theme_bw()

con_plot <- data %>% ggplot(mapping = aes(y = concavity_mean, x = diagnosis)) + 
  geom_boxplot(alpha = 0.5) +
  theme_bw()

(area_plot + sym_plot) / (txt_plot + con_plot)
```

```{r}
data %>% ggplot(mapping = aes(y = texture_se, x = diagnosis)) + 
  geom_boxplot(alpha = 0.5) +
  theme_bw()

data %>% ggplot(mapping = aes(y = concavity_worst, x = diagnosis)) + 
  geom_boxplot(alpha = 0.5) +
  theme_bw()
```


Itâ€™s important to point out that since 20 of the 30 predictors were computed from data, high multicollinearity exists in this dataset.

```{r fig.height=7}
corrplot(cor(data[,-1:-2]), method = "color", tl.col="black", tl.srt=45)
```

As we can see from the plot above, some predictors are highly correlated. One way we can deal with this is transforming the predictors using principal component analysis(PCA). We can then perform logistic regression on PCA transformed variables. However, the problem is that the model is not interpretable if we use PCA transformed variables. One of the biggest advantage of logistic regression is interpretability. By using PCA variables, we loose that advantage. So, instead, we use Variance inflation factor (VIF) as the criteria to remove multicollinearity.

## Model

#### Model specification

Since the variable we are interested in predicting in this problem is a binary variable, we chose to use a logistic regression model.

We assume that the binary response variable $Y_i$ follows a Bernoulli distribution with probability of success $p_i$ (success here means that the breast cancer case is malignant in the context of the problem). 

The link function we use:

$$
\text{logit}(p_i) = \log(\frac{p_i}{1-p_i})
$$

$$
\text{logit}(p_i) = \beta_0 + \beta_1X_1
$$


we can proceeds to prior specification and MCMC posterior inference of this logistic regression model.

We need a prior distribution


This is posterior distribution that stan_glm will draw from when using MCMC.

```{r cache=TRUE}
SEED=20200425
t_prior <- student_t(df = 1, location = 0, scale = 5)
post1 <- stan_glm(diagnosis ~ ., data = data[,-1],
                 family = binomial(link = "logit"), 
                 prior = t_prior, prior_intercept = t_prior, QR=TRUE,
                 seed = SEED, iter = 2000, chain = 1, refresh = 0)
```

```{r}
car::vif(post1)
```

Remove all predictors that have relatively high VIF values:

```{r cache=TRUE}
post2 <- stan_glm(diagnosis ~ symmetry_mean + texture_se +
                    smoothness_se + symmetry_se + smoothness_mean, data = data[,-1],
                 family = binomial(link = "logit"), 
                 prior = t_prior, prior_intercept = t_prior, QR=TRUE,
                 seed = SEED, iter = 2000, chain = 1, refresh = 0)
```

```{r}
post3 <- stan_glm(diagnosis ~ symmetry_mean + texture_mean +
                    area_mean + concavity_mean, data = data[,-1],
                 family = binomial(link = "logit"), 
                 prior = t_prior, prior_intercept = t_prior, QR=TRUE,
                 seed = SEED, iter = 2000, chain = 1, refresh = 0)
```

Model coefficients and CI:
```{r}
round(coef(post3), 2)
round(posterior_interval(post3, prob = 0.9), 2)
car::vif(post3)
```

## Model diagnostics

```{r}
#color_scheme_set("red")
#mcmc_trace(post2, pars = c("texture_se", "symmetry_mean", "smoothness_se", "smoothness_mean"))
```

```{r}
#mcmc_acf(post2, pars = c("texture_se", "symmetry_mean", "smoothness_se", "smoothness_mean"), lags = 10)
```

```{r}
color_scheme_set("red")
mcmc_trace(post3, pars = c("texture_mean", "symmetry_mean", "area_mean", "concavity_mean"))
```

```{r}
mcmc_acf(post3, pars = c("texture_mean", "symmetry_mean", "area_mean", "concavity_mean"), lags = 10)
```

#### Posterior predictive diagnostics

For posterior predictive checks, we used the 1000 posterior samples generated during the MCMC sampling to get our final prediction. Since each posterior sample contains predictions for all 569 observations, for each observation, we calculated the proportion of samples that are predicted to be malignant. If the proportion is above 0.5, then we conclude that this observation is a malignant case and benign otherwise. We can then generate a confusion matrix based on the final prediction to evaluate the performance of our model.

```{r}
pred <- posterior_predict(post3)
pr <- factor(as.integer(colMeans(pred) >= 0.5))
levels(pr) <- c("B", "M")
caret::confusionMatrix(pr, data$diagnosis)[2]
caret::confusionMatrix(pr, data$diagnosis)$overall["Accuracy"]
```

The accuracy of our predictions is about 92.8%. Even though our posterior predictive analysis shows that the model gives us very accurate predictions, there might be overfitting issues since we are testing on the training set, which is the dataset we used to fit our model earlier. So, to further test our posterior predictions and see if overfitting exists, we decided to use the `loo` package to perform a leave-one-out cross validation test.

```{r message=FALSE, warning=FALSE}
loo1 <- loo(post3, save_psis = TRUE)
ploo <- E_loo(pred, loo1$psis_object, type="mean", log_ratios = -log_lik(post3))$value
#round(mean(xor(ploo>0.5,as.integer(y==0))),2)
mean(xor(ploo>0.5,as.integer(data$diagnosis=="B")))
#mean( (ploo>0.5 & as.integer(data$diagnosis=="M")) | (ploo<0.5 & as.integer(data$diagnosis=="B")) )
```

The result shows an accuracy of 92.6%, which is very close to the accuracy we got earlier when we tested the predictions using the training set. This indicates that our model indeed is a good fit.


## Results


## Conclusion

